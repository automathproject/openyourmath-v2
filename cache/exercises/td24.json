{
  "uuid": "td24",
  "title": "Loi exponentielle",
  "chapter": "",
  "subchapter": "",
  "theme": "",
  "difficulty": null,
  "author": "",
  "organization": "",
  "video_id": "",
  "created_at": "2025-03-20",
  "updated_at": "2025-08-22T12:41:23.198Z",
  "content": [
    {
      "id": "block_1",
      "type": "text",
      "latex": "On considère un échantillon i.i.d. $(X_i)_{1 \\leq i \\leq n}$ avec $n \\geq 3$ et $X_1$ de densité $f_\\theta(x) = \\theta \\exp(-\\theta x) \\mathbf{1}_{[0,+\\infty[}(x)$, où $\\theta > 0$ est le paramètre inconnu.\\\\\nDans cet exercice on pourra utiliser le résultat suivant:\\\\\n\\emph{\nSoit $(X_n)$ une suite de variables aléatoires. Supposons qu'il existe un réel $a$ et une variable $X$ tels que:\n$$\\sqrt{n}(X_n-a) \\xrightarrow[n\\to\\infty]{\\mathcal{L}} X.$$\nSi $g$ est une fonction dérivable en $a$, alors \n$$\\sqrt{n}(g(X_n)-g(a)) \\xrightarrow[n\\to\\infty]{\\mathcal{L}} g'(a)X.$$\n}",
      "html": "<p>On considère un échantillon i.i.d. <span>\\((X_i)_{1 \\leq i \\leq n}\\)</span> avec <span>\\(n \\geq 3\\)</span> et <span>\\(X_1\\)</span> de densité <span>\\(f_\\theta(x) = \\theta \\exp(-\\theta x) \\mathbf{1}_{[0,+\\infty[}(x)\\)</span>, où <span>\\(\\theta &gt; 0\\)</span> est le paramètre inconnu.<br />\nDans cet exercice on pourra utiliser le résultat suivant:<br />\n<em>Soit <span>\\((X_n)\\)</span> une suite de variables aléatoires. Supposons qu’il existe un réel <span>\\(a\\)</span> et une variable <span>\\(X\\)</span> tels que:\n<span>\\[\\sqrt{n}(X_n-a) \\xrightarrow[n\\to\\infty]{\\mathcal{L}} X.\\]</span>\nSi <span>\\(g\\)</span> est une fonction dérivable en <span>\\(a\\)</span>, alors\n<span>\\[\\sqrt{n}(g(X_n)-g(a)) \\xrightarrow[n\\to\\infty]{\\mathcal{L}} g&#39;(a)X.\\]</span></em></p>",
      "order": 1
    },
    {
      "id": "block_2",
      "type": "question",
      "latex": "On propose d’estimer $\\theta$ par $Y_n = \\frac{n}{ \\sum_{i=1}^n X_i}$. \\\\\n(a) Montrer que la v.a. $Y_n$ est bien définie.\\\\\n(b) Expliquer pourquoi il est logique de choisir $Y_n$ comme estimateur de $\\theta$. \\\\\n(c) Déterminer la loi limite de $\\sqrt{n}(Y_n - \\theta)$. \\\\\n(d) Donner la loi de $\\sum_{i=1}^n X_i$. En déduire la valeur de $E[(Y_n - \\theta)^2]$.",
      "html": "<p>On propose d’estimer <span>\\(\\theta\\)</span> par <span>\\(Y_n = \\frac{n}{ \\sum_{i=1}^n X_i}\\)</span>.<br />\n(a) Montrer que la v.a. <span>\\(Y_n\\)</span> est bien définie.<br />\n(b) Expliquer pourquoi il est logique de choisir <span>\\(Y_n\\)</span> comme estimateur de <span>\\(\\theta\\)</span>.<br />\n(c) Déterminer la loi limite de <span>\\(\\sqrt{n}(Y_n - \\theta)\\)</span>.<br />\n(d) Donner la loi de <span>\\(\\sum_{i=1}^n X_i\\)</span>. En déduire la valeur de <span>\\(E[(Y_n - \\theta)^2]\\)</span>.</p>",
      "order": 2
    },
    {
      "id": "block_3",
      "type": "indication",
      "latex": "",
      "html": "",
      "order": 3
    },
    {
      "id": "block_4",
      "type": "reponse",
      "latex": "(a) La v.a. $Y_n$ est bien définie sur l'événement $\\{\\sum_{i=1}^n X_i > 0\\}$ qui est de probabilité 1 car les $X_i$ sont strictement positifs p.s. (la loi de $X_i$ ne charge pas 0 car elle est absolument continue par rapport à la mesure de Lebesgue). Ainsi, $Y_n$ est bien définie p.s.\n\n(b) Les v.a. $X_i$ suivent une loi exponentielle $\\mathcal{E}(\\theta)$. On a ainsi $E(X_1) = 1/\\theta$. La loi des grands nombres appliquée aux $X_i$ (i.i.d. et intégrables) montre que $\\overline{X}_n$ converge p.s. vers $1/\\theta > 0$. Par le théorème de continuité, $Y_n = 1/\\overline{X}_n$ converge p.s. vers $\\theta$. Cela indique que $Y_n = 1/\\overline{X}_n$ est un estimateur consistant et donc raisonnable de $\\theta$. Ce dernier correspond à l'estimateur par la méthode des moments.\n\n(c) Comme $\\text{Var}(X_1) = 1/\\theta^2 < \\infty$, le TCL donne que \n$$\\sqrt{n}(\\overline{X}_n - E(X_1)) \\xrightarrow[n\\to\\infty]{\\mathcal{L}} \\mathcal{N}(0, \\sigma(X_1))$$\nc'est-à-dire \n$$\\sqrt{n}(\\overline{X}_n - 1/\\theta) \\xrightarrow[n\\to\\infty]{\\mathcal{L}} \\mathcal{N}(0, 1/\\theta).$$\nÀ présent, on utilise le résultat admis avec la fonction $g:\\,x \\mapsto 1/x$, qui est bien dérivable sur $\\mathbb{R}^*_+$ de dérivée en $1/\\theta$ égale à $-1/(1/\\theta)^2 = -\\theta^2$ : \n$$\\sqrt{n}(Y_n - \\theta) \\xrightarrow[n\\to\\infty]{\\mathcal{L}} \\mathcal{N}(0, \\theta).$$\n\n(d) Les $X_i$ sont indépendantes et puisque chaque $X_i$ est de loi $\\mathcal{E}(\\theta) = \\gamma(1, \\theta)$,où $\\gamma(n,\\theta)$ est la densité de probabilité donnée par:\n$$\n\\gamma(n,\\theta)(x) = \\frac{\\theta^n}{\\Gamma(n)} x^{n-1} \\exp(-\\theta x)\\mathbf{1}_{[0,+\\infty[}(x),\n$$\nOn peut montrer que $\\sum_{i=1}^n X_i \\sim \\gamma(n, \\theta)$.\\\\\nEn effet la fonction caractéristique associée à la loi $X:=\\gamma(n,\\theta)$ est $\\varphi_{X}(t)=\\frac{1}{(1-it/\\theta)^{n}}$. Donc par exemple pour $X \\sim \\gamma(n_1,\\theta)$ et  $Y \\sim \\gamma(n_2,\\theta)$ indépendante on a:\n$$\\varphi_{X+Y}(t)=\\frac{1}{(1-it/\\theta)^{n_1}}\\times \\frac{1}{(1-it/\\theta)^{n_2}}=\\frac{1}{(1-it/\\theta)^{n_1+n_2}}, $$\net ainsi $X+Y \\sim \\gamma(n_1+n_2,\\theta)$ et le résultat pour une somme de $n$ v.a. indépendantes suit.\\\\\nOn en déduit $E(Y_n)$, $\\text{Var}(Y_n)$ et $E[(Y_n - \\theta)^2]$ :\n$$\\begin{align*}\nE(Y_n) &= E\\left(\\frac{n}{\\sum_{i=1}^n X_i}\\right) = nE\\left(\\frac{1}{\\sum_{i=1}^n X_i}\\right) \\\\\n&= n\\int_{\\mathbb{R}_+} \\frac{1}{x} \\frac{\\theta^n}{\\Gamma(n)} x^{n-1}\\exp(-\\theta x)dx \\\\\n&= \\frac{n\\theta}{\\Gamma(n)}\\int_{\\mathbb{R}_+} u^{n-2}\\exp(-u)du \\\\\n&= \\frac{n\\theta}{\\Gamma(n)}\\Gamma(n-1) = \\frac{n\\theta}{n-1},\n\\end{align*}$$\nen posant $u = \\theta x$ et en reconnaissant sous la dernière intégrale la densité de la $\\gamma(n-1, 1)$, au facteur $\\Gamma(n-1)$ près.\n\nOn obtient de la même manière :\n$$\\begin{align*}\nE(Y_n^2) &= \\frac{n^2\\theta^2}{\\Gamma(n)}\\Gamma(n-2) = \\frac{n^2\\theta^2}{(n-1)(n-2)},\n\\end{align*}$$\ndonc, sans même passer par la décomposition biais-variance,\n$$\\begin{align*}\nEQM(Y_n) = E[(Y_n - \\theta)^2] = E[Y_n^2] - 2\\theta E[Y_n] + \\theta^2 = \\frac{n+2}{(n-1)(n-2)}\\theta^2.\n\\end{align*}$$",
      "html": "<p>(a) La v.a. <span>\\(Y_n\\)</span> est bien définie sur l’événement <span>\\(\\{\\sum_{i=1}^n X_i &gt; 0\\}\\)</span> qui est de probabilité 1 car les <span>\\(X_i\\)</span> sont strictement positifs p.s. (la loi de <span>\\(X_i\\)</span> ne charge pas 0 car elle est absolument continue par rapport à la mesure de Lebesgue). Ainsi, <span>\\(Y_n\\)</span> est bien définie p.s.</p>\n<p>(b) Les v.a. <span>\\(X_i\\)</span> suivent une loi exponentielle <span>\\(\\mathcal{E}(\\theta)\\)</span>. On a ainsi <span>\\(E(X_1) = 1/\\theta\\)</span>. La loi des grands nombres appliquée aux <span>\\(X_i\\)</span> (i.i.d. et intégrables) montre que <span>\\(\\overline{X}_n\\)</span> converge p.s. vers <span>\\(1/\\theta &gt; 0\\)</span>. Par le théorème de continuité, <span>\\(Y_n = 1/\\overline{X}_n\\)</span> converge p.s. vers <span>\\(\\theta\\)</span>. Cela indique que <span>\\(Y_n = 1/\\overline{X}_n\\)</span> est un estimateur consistant et donc raisonnable de <span>\\(\\theta\\)</span>. Ce dernier correspond à l’estimateur par la méthode des moments.</p>\n<p>(c) Comme <span>\\(\\text{Var}(X_1) = 1/\\theta^2 &lt; \\infty\\)</span>, le TCL donne que\n<span>\\[\\sqrt{n}(\\overline{X}_n - E(X_1)) \\xrightarrow[n\\to\\infty]{\\mathcal{L}} \\mathcal{N}(0, \\sigma(X_1))\\]</span>\nc’est-à-dire\n<span>\\[\\sqrt{n}(\\overline{X}_n - 1/\\theta) \\xrightarrow[n\\to\\infty]{\\mathcal{L}} \\mathcal{N}(0, 1/\\theta).\\]</span>\nÀ présent, on utilise le résultat admis avec la fonction <span>\\(g:\\,x \\mapsto 1/x\\)</span>, qui est bien dérivable sur <span>\\(\\mathbb{R}^*_+\\)</span> de dérivée en <span>\\(1/\\theta\\)</span> égale à <span>\\(-1/(1/\\theta)^2 = -\\theta^2\\)</span> :\n<span>\\[\\sqrt{n}(Y_n - \\theta) \\xrightarrow[n\\to\\infty]{\\mathcal{L}} \\mathcal{N}(0, \\theta).\\]</span></p>\n<p>(d) Les <span>\\(X_i\\)</span> sont indépendantes et puisque chaque <span>\\(X_i\\)</span> est de loi <span>\\(\\mathcal{E}(\\theta) = \\gamma(1, \\theta)\\)</span>,où <span>\\(\\gamma(n,\\theta)\\)</span> est la densité de probabilité donnée par:\n<span>\\[\\gamma(n,\\theta)(x) = \\frac{\\theta^n}{\\Gamma(n)} x^{n-1} \\exp(-\\theta x)\\mathbf{1}_{[0,+\\infty[}(x),\\]</span>\nOn peut montrer que <span>\\(\\sum_{i=1}^n X_i \\sim \\gamma(n, \\theta)\\)</span>.<br />\nEn effet la fonction caractéristique associée à la loi <span>\\(X:=\\gamma(n,\\theta)\\)</span> est <span>\\(\\varphi_{X}(t)=\\frac{1}{(1-it/\\theta)^{n}}\\)</span>. Donc par exemple pour <span>\\(X \\sim \\gamma(n_1,\\theta)\\)</span> et <span>\\(Y \\sim \\gamma(n_2,\\theta)\\)</span> indépendante on a:\n<span>\\[\\varphi_{X+Y}(t)=\\frac{1}{(1-it/\\theta)^{n_1}}\\times \\frac{1}{(1-it/\\theta)^{n_2}}=\\frac{1}{(1-it/\\theta)^{n_1+n_2}},\\]</span>\net ainsi <span>\\(X+Y \\sim \\gamma(n_1+n_2,\\theta)\\)</span> et le résultat pour une somme de <span>\\(n\\)</span> v.a. indépendantes suit.<br />\nOn en déduit <span>\\(E(Y_n)\\)</span>, <span>\\(\\text{Var}(Y_n)\\)</span> et <span>\\(E[(Y_n - \\theta)^2]\\)</span> :\n<span>\\[\\begin{align*}\nE(Y_n) &amp;= E\\left(\\frac{n}{\\sum_{i=1}^n X_i}\\right) = nE\\left(\\frac{1}{\\sum_{i=1}^n X_i}\\right) \\\\\n&amp;= n\\int_{\\mathbb{R}_+} \\frac{1}{x} \\frac{\\theta^n}{\\Gamma(n)} x^{n-1}\\exp(-\\theta x)dx \\\\\n&amp;= \\frac{n\\theta}{\\Gamma(n)}\\int_{\\mathbb{R}_+} u^{n-2}\\exp(-u)du \\\\\n&amp;= \\frac{n\\theta}{\\Gamma(n)}\\Gamma(n-1) = \\frac{n\\theta}{n-1},\n\\end{align*}\\]</span>\nen posant <span>\\(u = \\theta x\\)</span> et en reconnaissant sous la dernière intégrale la densité de la <span>\\(\\gamma(n-1, 1)\\)</span>, au facteur <span>\\(\\Gamma(n-1)\\)</span> près.</p>\n<p>On obtient de la même manière :\n<span>\\[\\begin{align*}\nE(Y_n^2) &amp;= \\frac{n^2\\theta^2}{\\Gamma(n)}\\Gamma(n-2) = \\frac{n^2\\theta^2}{(n-1)(n-2)},\n\\end{align*}\\]</span>\ndonc, sans même passer par la décomposition biais-variance,\n<span>\\[\\begin{align*}\nEQM(Y_n) = E[(Y_n - \\theta)^2] = E[Y_n^2] - 2\\theta E[Y_n] + \\theta^2 = \\frac{n+2}{(n-1)(n-2)}\\theta^2.\n\\end{align*}\\]</span></p>",
      "order": 4
    },
    {
      "id": "block_5",
      "type": "question",
      "latex": "Pour estimer $\\theta$, on propose d’utiliser $Z_n = \\frac{n-1}{n} Y_n$.\\\\\n     (a) $Z_n$ vérifie-t-il des propriétés de convergence similaires à celles de $Y_n$ ?\\\\\n     (b) Qui de $Y_n$ ou $Z_n$ choisiriez-vous pour estimer $\\theta$ ?",
      "html": "<p>Pour estimer <span>\\(\\theta\\)</span>, on propose d’utiliser <span>\\(Z_n = \\frac{n-1}{n} Y_n\\)</span>.<br />\n(a) <span>\\(Z_n\\)</span> vérifie-t-il des propriétés de convergence similaires à celles de <span>\\(Y_n\\)</span> ?<br />\n(b) Qui de <span>\\(Y_n\\)</span> ou <span>\\(Z_n\\)</span> choisiriez-vous pour estimer <span>\\(\\theta\\)</span> ?</p>",
      "order": 5
    },
    {
      "id": "block_6",
      "type": "indication",
      "latex": "",
      "html": "",
      "order": 6
    },
    {
      "id": "block_7",
      "type": "reponse",
      "latex": "Les propriétés asymptotiques de $Z_n$ sont similaires à celles de $Y_n$ : comme $\\frac{n-1}{n} \\xrightarrow[n\\to\\infty]{} 1$ et $Y_n \\xrightarrow[n\\to\\infty]{p.s.} \\theta$, on a bien sûr $Z_n \\xrightarrow[n\\to\\infty]{p.s.} \\theta$. De plus, comme $\\sqrt{n}(Y_n - \\theta) \\xrightarrow[n\\to\\infty]{\\mathcal{L}} \\mathcal{N}(0, \\theta)$, nous avons \n\\begin{align}\n\\sqrt{n}(Z_n - \\theta) &= \\sqrt{n}(Y_n - \\theta) - \\sqrt{n}(Y_n - Z_n)\\\\\n&= \\sqrt{n}(Y_n - \\theta) - \\frac{Y_n}{\\sqrt{n}}\\\\\n&\\xrightarrow[n\\to\\infty]{\\mathcal{L}} \\mathcal{N}(0, \\theta),\n\\end{align}\noù nous avons utilisé le lemme de Slutsky car $\\frac{Y_n}{\\sqrt{n}}$ converge en probabilité vers $0$ (constante).\n\n(b) On calcule le risque de $Z_n$ :\n\\begin{align}\nE(Z_n) &= \\frac{n-1}{n}E(Y_n) = \\theta \\quad \\text{(estimateur sans biais)}\\\\\nE(Z_n^2) &= \\left(\\frac{n-1}{n}\\right)^2 E(Y_n^2) = \\frac{n-1}{n-2}\\theta^2\n\\end{align}\n\\begin{align}\nEQM(Z_n) &= E[(Z_n - \\theta)^2] = E[Z_n^2] - 2\\theta E[Z_n] + \\theta^2 =\\frac{\\theta^2}{n-2}.\n\\end{align}\n\nOn voit que l'écart quadratique moyen de $Z_n$ est inférieur à celui de $Y_n$ (pour toute valeur de $\\theta$, puisque $\\frac{n+2}{n-1} > 1$), l'estimateur $Z_n$ est donc un peu meilleur au sens du risque quadratique. La différence devient cependant négligeable lorsque $n$ grandit.",
      "html": "<p>Les propriétés asymptotiques de <span>\\(Z_n\\)</span> sont similaires à celles de <span>\\(Y_n\\)</span> : comme <span>\\(\\frac{n-1}{n} \\xrightarrow[n\\to\\infty]{} 1\\)</span> et <span>\\(Y_n \\xrightarrow[n\\to\\infty]{p.s.} \\theta\\)</span>, on a bien sûr <span>\\(Z_n \\xrightarrow[n\\to\\infty]{p.s.} \\theta\\)</span>. De plus, comme <span>\\(\\sqrt{n}(Y_n - \\theta) \\xrightarrow[n\\to\\infty]{\\mathcal{L}} \\mathcal{N}(0, \\theta)\\)</span>, nous avons\n<span>\\[\\begin{aligned}\n\\sqrt{n}(Z_n - \\theta) &amp;= \\sqrt{n}(Y_n - \\theta) - \\sqrt{n}(Y_n - Z_n)\\\\\n&amp;= \\sqrt{n}(Y_n - \\theta) - \\frac{Y_n}{\\sqrt{n}}\\\\\n&amp;\\xrightarrow[n\\to\\infty]{\\mathcal{L}} \\mathcal{N}(0, \\theta),\\end{aligned}\\]</span>\noù nous avons utilisé le lemme de Slutsky car <span>\\(\\frac{Y_n}{\\sqrt{n}}\\)</span> converge en probabilité vers <span>\\(0\\)</span> (constante).</p>\n<p>(b) On calcule le risque de <span>\\(Z_n\\)</span> :\n<span>\\[\\begin{aligned}\nE(Z_n) &amp;= \\frac{n-1}{n}E(Y_n) = \\theta \\quad \\text{(estimateur sans biais)}\\\\\nE(Z_n^2) &amp;= \\left(\\frac{n-1}{n}\\right)^2 E(Y_n^2) = \\frac{n-1}{n-2}\\theta^2\\end{aligned}\\]</span>\n<span>\\[\\begin{aligned}\nEQM(Z_n) &amp;= E[(Z_n - \\theta)^2] = E[Z_n^2] - 2\\theta E[Z_n] + \\theta^2 =\\frac{\\theta^2}{n-2}.\\end{aligned}\\]</span></p>\n<p>On voit que l’écart quadratique moyen de <span>\\(Z_n\\)</span> est inférieur à celui de <span>\\(Y_n\\)</span> (pour toute valeur de <span>\\(\\theta\\)</span>, puisque <span>\\(\\frac{n+2}{n-1} &gt; 1\\)</span>), l’estimateur <span>\\(Z_n\\)</span> est donc un peu meilleur au sens du risque quadratique. La différence devient cependant négligeable lorsque <span>\\(n\\)</span> grandit.</p>",
      "order": 7
    },
    {
      "id": "block_8",
      "type": "question",
      "latex": "Soit $\\alpha \\in ]0, 1[$. Donner un intervalle de confiance bilatère de niveau asymptotique $(1-\\alpha)$ pour $\\theta$.",
      "html": "<p>Soit <span>\\(\\alpha \\in ]0, 1[\\)</span>. Donner un intervalle de confiance bilatère de niveau asymptotique <span>\\((1-\\alpha)\\)</span> pour <span>\\(\\theta\\)</span>.</p>",
      "order": 8
    },
    {
      "id": "block_9",
      "type": "indication",
      "latex": "",
      "html": "",
      "order": 9
    },
    {
      "id": "block_10",
      "type": "reponse",
      "latex": "Puisque $\\frac{\\sqrt{n}}{\\theta}(Z_n - \\theta) \\xrightarrow[n\\to\\infty]{\\mathcal{L}} \\mathcal{N}(0,1)$ nous avons:\n    \n   $$\nP \\left(- \\Phi^{-1}(1 - \\alpha/2) < \\sqrt{n}(Z_n/\\theta-1) < \\Phi^{-1}(1 - \\alpha/2) \\right) \\xrightarrow[n\\to\\infty]{}  1 - \\alpha.\n$$    \n  On obtient un intervalle de confiance asymptotique  en réorganisant les termes de l'inégalité:  \n    $$\\big[\\frac{Z_n}{1 + \\Phi^{-1}(1-\\alpha/2)/\\sqrt{n}}, \\frac{Z_n}{1 - \\Phi^{-1}(1-\\alpha/2)/\\sqrt{n}}\n    \\big]\n     $$",
      "html": "<p>Puisque <span>\\(\\frac{\\sqrt{n}}{\\theta}(Z_n - \\theta) \\xrightarrow[n\\to\\infty]{\\mathcal{L}} \\mathcal{N}(0,1)\\)</span> nous avons:</p>\n<p><span>\\[P \\left(- \\Phi^{-1}(1 - \\alpha/2) &lt; \\sqrt{n}(Z_n/\\theta-1) &lt; \\Phi^{-1}(1 - \\alpha/2) \\right) \\xrightarrow[n\\to\\infty]{}  1 - \\alpha.\\]</span>\nOn obtient un intervalle de confiance asymptotique en réorganisant les termes de l’inégalité:\n<span>\\[\\big[\\frac{Z_n}{1 + \\Phi^{-1}(1-\\alpha/2)/\\sqrt{n}}, \\frac{Z_n}{1 - \\Phi^{-1}(1-\\alpha/2)/\\sqrt{n}}\n    \\big]\\]</span></p>",
      "order": 10
    },
    {
      "id": "block_11",
      "type": "question",
      "latex": "Proposer un test de niveau asymptotique $\\alpha$ pour tester $H_0 : \\theta \\geq 1$ contre $H_1 : \\theta < 1$.",
      "html": "<p>Proposer un test de niveau asymptotique <span>\\(\\alpha\\)</span> pour tester <span>\\(H_0 : \\theta \\geq 1\\)</span> contre <span>\\(H_1 : \\theta &lt; 1\\)</span>.</p>",
      "order": 11
    },
    {
      "id": "block_12",
      "type": "indication",
      "latex": "",
      "html": "",
      "order": 12
    },
    {
      "id": "block_13",
      "type": "reponse",
      "latex": "Pour tout $\\theta > 0$, par le même raisonnement que ci-dessus, un intervalle de confiance unilatéral de\nniveau asymptotique $1 - \\alpha$ pour $\\theta$ est (en prenant comme convention $]a, b] = \\emptyset$ pour $b \\leq a$) :\n\n$$\n\\left] 0, \\frac{Z_n}{1 + \\Phi^{-1}(\\alpha) / \\sqrt{n}} \\right].\n$$\n\nLe lien entre intervalles de confiance (asymptotiques) et tests (asymptotiques) assure donc que le\ntest consistant à rejeter $H_0$ ssi\n\n$$\n[1, +\\infty[ \\cap \\left] 0, \\frac{Z_n}{1 + \\Phi^{-1}(\\alpha) / \\sqrt{n}} \\right] = \\emptyset\n$$\n\nest de niveau asymptotique $\\alpha$. Dit autrement, on rejette $H_0$ ssi\n\n$$\nZ_n < 1 + \\frac{\\Phi^{-1}(\\alpha)}{\\sqrt{n}}.\n$$\n\\textbf{Remarque 1} : en mettant le test sous cette dernière forme, on a supposé que\n$1 + \\frac{\\Phi^{-1}(\\alpha)}{\\sqrt{n}} > 0$, or à $n$ fixé petit ceci n’est pas toujours vrai puisque pour les valeurs de $\\alpha$ courantes (i.e. inférieures à $1/2$, par exemple $\\alpha = 5\\%$), on a $\\Phi^{-1}(\\alpha) < 0$ (par exemple $\\Phi^{-1}(0.05) \\approx -1.64$), donc pour $n < \\Phi^{-1}(\\alpha)^2$\n(par exemple $n < \\Phi^{-1}(0.05)^2 \\approx 2.7$), il est clair que $1 + \\frac{\\Phi^{-1}(\\alpha)}{\\sqrt{n}} < 0$ et le passage à la dernière forme n’est pas licite. \n\nNéanmoins, il faut bien garder à l’esprit que tout ce qu’on dit n’est valable que pour $n \\to \\infty$, or quel que soit $\\alpha \\in ]0,1[$ on a bien asymptotiquement $1 + \\frac{\\Phi^{-1}(\\alpha)}{\\sqrt{n}} > 0$, donc le passage à la dernière expression est correct. Quoi qu’il en soit, si on veut éviter toute discussion, il suffit de garder le test sous la forme : rejet de $H_0$ ssi\n\n$$\n\\frac{Z_n}{1 + \\Phi^{-1}(\\alpha) / \\sqrt{n}} < 1.\n$$\n\n\\textbf{Remarque 2} : pour construire un test, il suffit que l’intervalle de confiance soit vrai sous $H_0$, c’est-à-dire pour tout $\\theta \\in \\Theta_0 = [1, \\infty)$, et pas nécessairement sur tout $\\mathbb{R}$. On aurait donc très bien pu écrire que pour tout $\\theta \\geq 1$, \n\n$$\n\\left] 0, \\frac{Z_n}{1+ \\Phi^{-1}(\\alpha) / \\sqrt{n}} \\right]\n$$\n\nest un intervalle de confiance de niveau asymptotique $1-\\alpha$ pour $\\theta$ sous $H_0$, et construire le test par intersection avec $[1, +\\infty[$, ce qui sur cet exemple serait revenu au même. La situation typique où il est plus judicieux de construire l’intervalle de confiance en tenant compte du fait que $\\theta \\in \\Theta_0$ est celle où $\\Theta_0 = \\{\\theta_0\\}$, c’est-à-dire que $H_0$ est une hypothèse simple (cf. question suivante).",
      "html": "<p>Pour tout <span>\\(\\theta &gt; 0\\)</span>, par le même raisonnement que ci-dessus, un intervalle de confiance unilatéral de\nniveau asymptotique <span>\\(1 - \\alpha\\)</span> pour <span>\\(\\theta\\)</span> est (en prenant comme convention <span>\\(]a, b] = \\emptyset\\)</span> pour <span>\\(b \\leq a\\)</span>) :</p>\n<p><span>\\[\\left] 0, \\frac{Z_n}{1 + \\Phi^{-1}(\\alpha) / \\sqrt{n}} \\right].\\]</span></p>\n<p>Le lien entre intervalles de confiance (asymptotiques) et tests (asymptotiques) assure donc que le\ntest consistant à rejeter <span>\\(H_0\\)</span> ssi</p>\n<p><span>\\[[1, +\\infty[ \\cap \\left] 0, \\frac{Z_n}{1 + \\Phi^{-1}(\\alpha) / \\sqrt{n}} \\right] = \\emptyset\\]</span></p>\n<p>est de niveau asymptotique <span>\\(\\alpha\\)</span>. Dit autrement, on rejette <span>\\(H_0\\)</span> ssi</p>\n<p><span>\\[Z_n &lt; 1 + \\frac{\\Phi^{-1}(\\alpha)}{\\sqrt{n}}.\\]</span>\n<strong>Remarque 1</strong> : en mettant le test sous cette dernière forme, on a supposé que\n<span>\\(1 + \\frac{\\Phi^{-1}(\\alpha)}{\\sqrt{n}} &gt; 0\\)</span>, or à <span>\\(n\\)</span> fixé petit ceci n’est pas toujours vrai puisque pour les valeurs de <span>\\(\\alpha\\)</span> courantes (i.e. inférieures à <span>\\(1/2\\)</span>, par exemple <span>\\(\\alpha = 5\\%\\)</span>), on a <span>\\(\\Phi^{-1}(\\alpha) &lt; 0\\)</span> (par exemple <span>\\(\\Phi^{-1}(0.05) \\approx -1.64\\)</span>), donc pour <span>\\(n &lt; \\Phi^{-1}(\\alpha)^2\\)</span>\n(par exemple <span>\\(n &lt; \\Phi^{-1}(0.05)^2 \\approx 2.7\\)</span>), il est clair que <span>\\(1 + \\frac{\\Phi^{-1}(\\alpha)}{\\sqrt{n}} &lt; 0\\)</span> et le passage à la dernière forme n’est pas licite.</p>\n<p>Néanmoins, il faut bien garder à l’esprit que tout ce qu’on dit n’est valable que pour <span>\\(n \\to \\infty\\)</span>, or quel que soit <span>\\(\\alpha \\in ]0,1[\\)</span> on a bien asymptotiquement <span>\\(1 + \\frac{\\Phi^{-1}(\\alpha)}{\\sqrt{n}} &gt; 0\\)</span>, donc le passage à la dernière expression est correct. Quoi qu’il en soit, si on veut éviter toute discussion, il suffit de garder le test sous la forme : rejet de <span>\\(H_0\\)</span> ssi</p>\n<p><span>\\[\\frac{Z_n}{1 + \\Phi^{-1}(\\alpha) / \\sqrt{n}} &lt; 1.\\]</span></p>\n<p><strong>Remarque 2</strong> : pour construire un test, il suffit que l’intervalle de confiance soit vrai sous <span>\\(H_0\\)</span>, c’est-à-dire pour tout <span>\\(\\theta \\in \\Theta_0 = [1, \\infty)\\)</span>, et pas nécessairement sur tout <span>\\(\\mathbb{R}\\)</span>. On aurait donc très bien pu écrire que pour tout <span>\\(\\theta \\geq 1\\)</span>,</p>\n<p><span>\\[\\left] 0, \\frac{Z_n}{1+ \\Phi^{-1}(\\alpha) / \\sqrt{n}} \\right]\\]</span></p>\n<p>est un intervalle de confiance de niveau asymptotique <span>\\(1-\\alpha\\)</span> pour <span>\\(\\theta\\)</span> sous <span>\\(H_0\\)</span>, et construire le test par intersection avec <span>\\([1, +\\infty[\\)</span>, ce qui sur cet exemple serait revenu au même. La situation typique où il est plus judicieux de construire l’intervalle de confiance en tenant compte du fait que <span>\\(\\theta \\in \\Theta_0\\)</span> est celle où <span>\\(\\Theta_0 = \\{\\theta_0\\}\\)</span>, c’est-à-dire que <span>\\(H_0\\)</span> est une hypothèse simple (cf. question suivante).</p>",
      "order": 13
    },
    {
      "id": "block_14",
      "type": "question",
      "latex": "Proposer un test de niveau asymptotique $\\alpha$ pour tester $H_0 : \\theta = 1$ contre $H_1 : \\theta \\neq 1$.",
      "html": "<p>Proposer un test de niveau asymptotique <span>\\(\\alpha\\)</span> pour tester <span>\\(H_0 : \\theta = 1\\)</span> contre <span>\\(H_1 : \\theta \\neq 1\\)</span>.</p>",
      "order": 14
    },
    {
      "id": "block_15",
      "type": "indication",
      "latex": "",
      "html": "",
      "order": 15
    },
    {
      "id": "block_16",
      "type": "reponse",
      "latex": "\\textbf{Sous $H_0$ : $\\theta = 1$}, on sait d’après ci-dessus que\n\n$$\n\\sqrt{n} (Z_n - 1) \\xrightarrow{\\mathcal{L}\t} \\mathcal{N}(0,1) \\quad \\text{lorsque } n \\to \\infty,\n$$\n\ndonc un intervalle de confiance bilatéral de niveau asymptotique $1 - \\alpha$ pour la valeur $1$ est\n\n$$\n\\left( Z_n - \\frac{\\Phi^{-1}(1 - \\alpha/2)}{\\sqrt{n}}, Z_n + \\frac{\\Phi^{-1}(1 - \\alpha/2)}{\\sqrt{n}} \\right).\n$$\n\nLe test consistant à rejeter $H_0$ ssi $1$ n’est pas dans cet intervalle est de niveau asymptotique $\\alpha$, ce qui revient à dire que l’on rejette $H_0$ ssi\n\n$$\n|Z_n - 1| > \\frac{\\Phi^{-1}(1 - \\alpha/2)}{\\sqrt{n}}.\n$$",
      "html": "<p><strong>Sous <span>\\(H_0\\)</span> : <span>\\(\\theta = 1\\)</span></strong>, on sait d’après ci-dessus que</p>\n<p><span>\\[\\sqrt{n} (Z_n - 1) \\xrightarrow{\\mathcal{L} } \\mathcal{N}(0,1) \\quad \\text{lorsque } n \\to \\infty,\\]</span></p>\n<p>donc un intervalle de confiance bilatéral de niveau asymptotique <span>\\(1 - \\alpha\\)</span> pour la valeur <span>\\(1\\)</span> est</p>\n<p><span>\\[\\left( Z_n - \\frac{\\Phi^{-1}(1 - \\alpha/2)}{\\sqrt{n}}, Z_n + \\frac{\\Phi^{-1}(1 - \\alpha/2)}{\\sqrt{n}} \\right).\\]</span></p>\n<p>Le test consistant à rejeter <span>\\(H_0\\)</span> ssi <span>\\(1\\)</span> n’est pas dans cet intervalle est de niveau asymptotique <span>\\(\\alpha\\)</span>, ce qui revient à dire que l’on rejette <span>\\(H_0\\)</span> ssi</p>\n<p><span>\\[|Z_n - 1| &gt; \\frac{\\Phi^{-1}(1 - \\alpha/2)}{\\sqrt{n}}.\\]</span></p>",
      "order": 16
    }
  ],
  "artifacts": {
    "tikz": [],
    "geogebra": [],
    "code": [],
    "video": null
  },
  "source_hash": "258be3603099da60e7740d84882c80f0b387878281426df52e217d010b63b400"
}